{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "fc8ccbab",
   "metadata": {},
   "source": [
    "# AI Security Homework 1\n",
    "\n",
    "- Lecturer: Sangkyun Lee\n",
    "### Due: April 10th (Wed) 2024, 11:59pm KST\n",
    "- Answer the following questions in <span style=\"color:red\">**English**</span>. Answers in other languages won't be accepted.\n",
    "- Submit your answer as <span style=\"color:red\">**a PDF file**</span> through blackboard (LMS)\n",
    "- <span style=\"color:red\">**Typeset**</span> your answers using equation editors (for example, in MS Word or HWP) or using LaTeX. Handwritten answers won't be accepted.\n",
    "- Total 35 points\n",
    "---"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b9707b0f",
   "metadata": {},
   "source": [
    "## Prediction Performance"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d2061822",
   "metadata": {},
   "source": [
    "Consider a classification problem where an input data X and its label Y are sampled from a (usually unknown) probability distribution, that is,\n",
    "\n",
    "$$\n",
    "  (X, Y) \\sim \\mathbb{P}(X,Y)\n",
    "$$"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "8cc95dec",
   "metadata": {},
   "source": [
    "Given a dataset $\\{(x_i, y_i)\\}_{i=1}^n$ consisting of $n$ data points sampled from $P(X,Y)$ and a classifier $h(x_i)$ that gives a prediction in the same form as $y_i$, we can think of different types of errors:"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "3992b8b6",
   "metadata": {},
   "source": [
    "- i) Training error rate: \n",
    "\n",
    "$$\\frac1n \\sum_{i=1}^n 1_{h(x_i) \\neq y_i}$$\n",
    "\n",
    "where $1_{event}$ is an indicator function that returns 1 of the event is true and 0 otherwise."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "5c031dca",
   "metadata": {},
   "source": [
    "- ii) Generation error: \n",
    "\n",
    "$$\n",
    " \\mathbb{E}_{(X,Y)} \\left[ 1_{h(X)\\neq Y} \\right] = \\int_{x,y} 1_{h(x)\\neq y} \\mathbb{P}(x,y) dxdy\n",
    "$$"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "5d49830a",
   "metadata": {},
   "source": [
    "#### 1 [4]. Explain what does the equation in i) computes.\n",
    "\n",
    "#### 2 [5]. How does the eq in i) differ from the eq in ii) ? Explain.\n",
    "\n",
    "#### 3 [6]. In pratical learning situation, we divide a given dataset into a train, a validation, and a test set. Explain the purpose of each dataset."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d62ee29f",
   "metadata": {},
   "source": [
    "---"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b17be1f1",
   "metadata": {},
   "source": [
    "## Logistic Regression"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "1cab6317",
   "metadata": {},
   "source": [
    "The following code is designed to solve the following logistic regression problem:"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "be7fdf74",
   "metadata": {},
   "source": [
    "$$\n",
    "\\min_{(w \\in \\mathbb R^p,b \\in \\mathbb R)} J(w,b) := - \\frac1n \\sum_{i=1}^n \\Big( y^{(i)} \\log \\sigma(w^T x^{(i)}+b) + (1-y^{(i)}) \\log (1-\\sigma(w^T x^{(i)}+b)) \\Big)\n",
    "$$\n",
    "\n",
    "where $p$ is the number of input features and $\\sigma(z^{(i)}) = \\frac{1}{1+\\exp(-z^{(i)})}$."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "6600adc8",
   "metadata": {},
   "source": [
    "#### 4 [10] Show the expression of $\\frac{\\partial J(w,b)}{\\partial b}$, showing the intermediate derivations. Discuss if code fragment below (marked as #### Problem 4 ####) implementing this computation makes sense.\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b09d53d2",
   "metadata": {},
   "outputs": [],
   "source": [
    "class LogisticRegressionGD:\n",
    "    \"\"\"Gradient descent-based logistic regression classifier.\n",
    "\n",
    "    Parameters\n",
    "    ------------\n",
    "    eta : float\n",
    "      Learning rate (between 0.0 and 1.0)\n",
    "    n_iter : int\n",
    "      Passes over the training dataset.\n",
    "    random_state : int\n",
    "      Random number generator seed for random weight\n",
    "      initialization.\n",
    "\n",
    "\n",
    "    Attributes\n",
    "    -----------\n",
    "    w_ : 1d-array\n",
    "      Weights after training.\n",
    "    b_ : Scalar\n",
    "      Bias unit after fitting.\n",
    "    losses_ : list\n",
    "       Log loss function values in each epoch.\n",
    "\n",
    "    \"\"\"\n",
    "    def __init__(self, eta=0.01, n_iter=50, random_state=1):\n",
    "        self.eta = eta\n",
    "        self.n_iter = n_iter\n",
    "        self.random_state = random_state\n",
    "\n",
    "    def fit(self, X, y):\n",
    "        \"\"\" Fit training data.\n",
    "\n",
    "        Parameters\n",
    "        ----------\n",
    "        X : {array-like}, shape = [n_examples, n_features]\n",
    "          Training vectors, where n_examples is the number of examples and\n",
    "          n_features is the number of features.\n",
    "        y : array-like, shape = [n_examples]\n",
    "          Target values.\n",
    "\n",
    "        Returns\n",
    "        -------\n",
    "        self : Instance of LogisticRegressionGD\n",
    "\n",
    "        \"\"\"\n",
    "        rgen = np.random.RandomState(self.random_state)\n",
    "        self.w_ = rgen.normal(loc=0.0, scale=0.01, size=X.shape[1])\n",
    "        self.b_ = np.float_(0.)\n",
    "        self.losses_ = []\n",
    "\n",
    "        for i in range(self.n_iter):\n",
    "            net_input = self.net_input(X)\n",
    "            output = self.activation(net_input)\n",
    "            errors = (y - output)\n",
    "            self.w_ += self.eta * X.T.dot(errors) / X.shape[0]\n",
    "            self.b_ += self.eta * errors.mean()                    ########### Problem 4 ###########\n",
    "            loss = (-y.dot(np.log(output)) - (1 - y).dot(np.log(1 - output))) / X.shape[0]\n",
    "            self.losses_.append(loss)\n",
    "        return self\n",
    "\n",
    "    def net_input(self, X):\n",
    "        \"\"\"Calculate net input\"\"\"\n",
    "        return np.dot(X, self.w_) + self.b_\n",
    "\n",
    "    def activation(self, z):\n",
    "        \"\"\"Compute logistic sigmoid activation\"\"\"\n",
    "        return 1. / (1. + np.exp(-np.clip(z, -250, 250)))\n",
    "\n",
    "    def predict(self, X):\n",
    "        \"\"\"Return class label after unit step\"\"\"\n",
    "        return np.where(self.activation(self.net_input(X)) >= 0.5, 1, 0)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "3865edbc",
   "metadata": {},
   "source": [
    "---"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "81b46be6",
   "metadata": {},
   "source": [
    "## SVM\n",
    "\n",
    "<img src='../ch03/images/03_09.png' align=\"middle\" width=1200>"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d9759bbf",
   "metadata": {},
   "source": [
    "#### 5 [10]. Consider an SVM, consisting of three hyperplanes:\n",
    "\n",
    "- Decision plane: $w^Tx + b = 0$\n",
    "- Two supporting planes: $w^Tx+b = +1$ and $w^Tx+b=-1$\n",
    "\n",
    "Show that the distance between two supporting planes (called the 'margin') can be formulated as $$ \\frac{2}{\\|w\\|_2}$$."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f7203638",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.8"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
